---
title: "Examples R"
date: 2018-12-21T11:55:05+03:00 
draft: true
description: ""
tags:
categories: examples, R
type: post
url:
author: "Mert Nuhoglu"
output:
  html_document:
    css: styles.css
blog: mertnuhoglu.com
resource_files:
- 
path: ~/projects/study/db/examples_r.Rmd
state: wip
---

``` R
vars01 = rio::import("/Users/mertnuhoglu/projects/bizqualify/data_bizqualify/BizQualify_v6_4_complete data dictionary_current_with financials__Dec 2018.xlsx", which = 1) %>%
	select(field_name) %>%
	mutate(field_name = tolower(field_name)) 
vars_df = bind_rows(vars01, vars02, vars03, vars04)
nrow(vars_df)
vars_f = setdiff(vars, "bq_data_run_date")
length(vars_f)
``` 

tbl or dplyr with sql

``` R
sql01 = "
(SELECT *
FROM data_20181220.all_data 
WHERE bq_company_address1_zip5 = '94303'
AND bq_year = 2018
ORDER BY bq_current_employees_plan DESC
LIMIT 100)
UNION ALL
(SELECT *
FROM data_20181220.all_data 
WHERE bq_company_address1_zip5 = '10010'
AND bq_year = 2018
ORDER BY bq_current_employees_plan DESC
LIMIT 100)
UNION ALL
(SELECT *
FROM data_20181220.all_data 
WHERE bq_company_address1_zip5 = '02451'
AND bq_year = 2018
ORDER BY bq_current_employees_plan DESC
LIMIT 100)
"
d01 = tbl(con, sql(sql01)) %>% collect
``` 

``` R
d02 = d01 %>%
	select_(.dots = vars_f)
write_csv(d02, path = "~/bizqualify_data/zoom_data_extract_20181221.csv", na = "")
``` 

### 01.01.a opt01: Import into Database Using R

Filter public companies into different files because current files are too big:

``` R
library(dplyr)
library(readr)
setwd("~/bizqualify_data")
d10 = read_csv("bq_hedge_funds_all_data_20181001.csv") 
d10b = d10 %>%
	filter(!is.na(bq_ticker))
write_csv(d10b, "public_20181001.csv", na="")
``` 

Import data into database using R:

``` R
library("RPostgreSQL")
source("~/BQ-data-run/config.R")
dbWriteTable(con, c("client", "altdg_sample"), d0, row.names=F)
``` 

Note that: `dbWriteTable` doesn't write the data when it has `append=F` and the table was created already. (default value)

### 01.02. Load data from database

Load both data sets from database now:

``` R
d10 = tbl(con, sql("SELECT * FROM data_20181001.public_all_data")) %>% collect
``` 

## Run a SQL query

``` bash
dbGetQuery(con, statement = paste0("
  insert into ", data_schema, ".master_variables
  select * from ", data_schema, ".master_variables_forecast
  where source = 'ez'
"))
``` 

opt02: use dplyr::tbl()

``` bash
d0 = tbl(con, sql("
	SELECT company_ein
  FROM data_20181202.master_variables
  WHERE extract(YEAR from plan_year_end_date) = 2014 and source = 'ez'
	")) %>% collect
nrow(d0)
``` 

#### 01.01.03 Compare company_ein Between Them

``` R
d2 = setdiff(d0$company_ein, d1$company_ein)
length(d2)
  ##> [1] 112087
d3 = setdiff(d1$company_ein, d0$company_ein)
length(d3)
  ##> [1] 124030
``` 

# select and filter

Compare with 20180903 and 20180806:

``` r
c01 = read_csv("~/bizqualify_data/data_20180903_ts.csv") %>%
nrow(c01)
  ##> [1] 6521042
c02 = c01 %>%
	select(company_ein, bq_ticker, bq_company_name, bq_comp_most_recent_filing_date, bq_year)
c03 = c02 %>%
	filter(bq_comp_most_recent_filing_date >= as.Date('2017-1-1')) %>%
	filter(bq_comp_most_recent_filing_date < as.Date('2018-1-1')) 
c04 = unique(c03$company_ein)
length(c04)
	##> [1] 334453
dif_11_09 = a04[!a04 %in% c04]
length(dif_11_09)
	##> [1] 15115
dif_09_11 = c04[!c04 %in% a04]
length(dif_09_11)
	##> [1] 87989
``` 

Let's compare 08 (august) to 09 (september)

``` r
dif_09_08 = c04[!c04 %in% d04]
length(dif_09_08)
	##> [1] 922
dif_08_09 = d04[!d04 %in% c04]
length(dif_08_09)
	##> [1] 63003
``` 

# Compare Lists of Lines or Words

``` r
> d09 = readLines("0903.txt")
> d10 = readLines("1027.txt")
> setdiff(d09, d10)
[1] "bq_industry_sub_sector_code bigint" "bq_sp_500_indicator text"
> setdiff(d10, d09)
 [1] "bq_company_legal_name_expanded text"              "bq_emp_contrib_pens_amt_a_us_m bigint"
 [3] "bq_growth_emp_contrib_pens_amt_a_p bigint"        "bq_growth_emp_contrib_pens_amt_a_p_us bigint"
``` 

# Mutate column types

``` r
bd10 = bd10 %>%
	mutate(bq_revenue = as.numeric(bq_revenue)) %>%
	mutate(bq_cogs = as.numeric(bq_cogs)) %>%
	mutate(bq_gross_profit = as.integer(bq_gross_profit)) %>%
	mutate(bq_total_assets = as.numeric(bq_total_assets)) %>%
	mutate(bq_profitability_score = as.integer(bq_profitability_score)) %>%
	mutate(bq_profitability_score_i_m = as.integer(bq_profitability_score_i_m)) %>%
	mutate(bq_profitability_score_p = as.integer(bq_profitability_score_p)) %>%
	mutate(bq_profitability_score_p_us = as.integer(bq_profitability_score_p_us)) %>%
	mutate(bq_profitability_score_r = as.integer(bq_profitability_score_r)) %>%
	mutate(bq_profitability_score_r_us = as.integer(bq_profitability_score_r_us)) %>%
	mutate(bq_profitability_score_us_m = as.integer(bq_profitability_score_us_m)) 
```

## Disable Scientific Notation in write_csv

Note that, write_csv formats big numbers that end with multiple zeros in scientific notation. But scientific numbers cannot be imported into Postgresql.

To disable scientific notation, there are alternative ways:

``` r
options(scipen = 999)
write_csv(bd11[270:290,], "~/bizqualify_data/tmp.csv", na = "")
options(digits=22) 
write.csv(bd11[270:290,], "~/bizqualify_data/tmp.csv", na = "")
write.csv(bd11, "~/bizqualify_data/data_20181105_ts2.csv", na = "", row.names = F)
write.csv(bd11[1:10000,], "~/bizqualify_data/tmp.csv", na = "", row.names = F)
```

## 06. Verify that These Are Unique Really - 2. iteration

Now, 

``` R
cnt = tbl(con, sql("SELECT * FROM data_20181202.companies_non_ts")) %>% collect
length(unique(cnt$company_ein))
  ##> [1] 1149644
nrow(cnt)
  ##> [1] 6583082
``` 

Check other variables

``` R
distinct(cnt, company_ein, bq_company_legal_name) %>% nrow()
  ##> [1] 1149644
``` 

## Setup Google Drive Access using R googledrive

``` bash
install.packages("devtools")
devtools::install_github("tidyverse/googlesheets4")
library(googledrive)
library(googlesheets4)
``` 

Upload the csv and simultaneously convert to a Sheet

``` bash
(iris_tempfile <- tempfile(pattern = "iris-", fileext = ".csv"))
write.csv(iris, iris_tempfile, row.names = FALSE)
(iris_ss <- drive_upload(iris_tempfile, type = "spreadsheet"))
``` 

``` bash
## visit the new Sheet in the browser, in an interactive session!
drive_browse(iris_ss)
``` 

## other

``` R
layout <- read.csv(paste0(raw_data_dir, file), stringsAsFactors = FALSE)
``` 

### Rmarkdown code blocks

``` {R include=F}
options(scipen = 999)
``` 

``` {R echo = F, message = F, warning = F}
library("RPostgreSQL")
library(dplyr)
library(readr)
library(knitr)

source("~/BQ-data-run/config.R")
``` 

``` {R echo=F}
sql = "
SELECT schemaname,relname,n_live_tup AS row_count
  FROM pg_stat_user_tables 
	WHERE schemaname = 'twosigma'
  ORDER BY n_live_tup DESC
"
dbGetQuery(con, sql)
``` 

``` {R echo=F}
sql = glue("
WITH unique_year_ticker AS (
	SELECT DISTINCT ON (bq_year, bq_ticker) bq_year, bq_ticker
	FROM {data_schema}.all_data
	ORDER BY bq_year, bq_ticker
)
SELECT bq_year, count(*) AS unique_bq_ticker
FROM unique_year_ticker 
GROUP BY bq_year
ORDER BY bq_year;
")
d01 = dbGetQuery(con, sql)
knitr::kable(d01)
``` 

### summary

``` R
summary(d0$percent_change_revenue)
  ##>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's
  ##>     0.00     0.00     0.00    11.69     0.00 40400.00        2
summary(d0$percent_change_employment)
  ##>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
  ##>     0.000     0.000     0.000     5.868     0.000 28100.000
``` 

### custom summary

``` R
f1 = d0 %>%
	filter(percent_change_employment != 0)
stat = list(
  total_no_companies = nrow(d0)
	, no_change_in_employment = nrow(d0) - nrow(f1)
	, changed = nrow(f1)
	, percent_changed = (nrow(f1)/nrow(d0)*100)
)
``` 

### rmarkdown report

``` bash
file=twosigma/report_twosigma_florida_20190211.Rmd
R -e 'rmarkdown::render("'"$file"'", "md_document")'
``` 

### other

show_query id=g_10794

``` R
d01 = tbl(con, sql(s)) 
vars = c("bq_ticker", "bq_company_name", "bq_revenue")
d02 = d01 %>%
	dplyr::select(vars) 
show_query(d02)
  ##> SELECT "bq_ticker", "bq_company_name", "bq_revenue"
  ##> FROM (SELECT bq_ticker, bq_company_name, bq_revenue
  ##> FROM data_20190203.all_data) "zzzduwkcqn"
``` 

### glue id=g_10793

``` R
library(glue)
schema = "data_20190203"
s = glue("
SELECT bq_ticker, bq_company_name, bq_revenue
FROM {schema}.all_data
")
``` 

### gather id=g_10792

``` r
library(tidyverse)
d0 = data_frame(
	a = c(1,1,2,2,3),
	b = c(5,6,5,8,9)
)
gather(d0, "var", "value")
  ##>    var   value
  ##>  1 a         1
  ##>  2 a         1
``` 

``` r
d0 = data_frame(
	a = c(1,1,2,2,3),
	b = c(5,6,5,8,9),
	c = letters[1:5]
)
gather(d0, "var", "value", -c)
  ##>    c     var   value
  ##>    <chr> <chr> <dbl>
  ##>  1 a     a         1
  ##>  2 b     a         1
gather(d0, key = "var", value = "value", -c)
gather(d0, key = "var", value = "value", c(a,b))
``` 

### spread id=g_10803

``` r
s1 = tibble::tribble(
  ~id,   ~key, ~value,
  1,   "a",  2,
  1,   "b",  3,
  2,   "a",  2
  ) %>%
  tidyr::spread(key, value)
s1
#> # A tibble: 2 x 3
#>      id     a     b
#> * <dbl> <dbl> <dbl>
#> 1     1     2     3
#> 2     2     2    NA
```

### building R packages id=g_10799

opt02: building on R console (for debugging)

``` bash
cd ~/projects/yuml2data/
Rs
``` 

``` r
library(devtools)
devtools::document()
devtools::build_vignettes()
devtools::load_all()
library(magrittr)
devtools::build()
devtools::install()
``` 

opt03: building on terminal

``` bash
cd ~/projects
R CMD build yuml2data
R CMD INSTALL yuml2data_0.1.0.tar.gz
``` 

### verifications for keys id=g_10797

check uniqueness

``` r
length(unique(d0$ticker)) == nrow(d0)
  ##> TRUE
``` 

What is the duplicate `name` value?

``` r
d0 %>%
  distinct(ticker, name) %>%
  group_by(name) %>%
  summarise(n = n()) %>%
	filter(n > 1)
  ##>   name                n
  ##>   <chr>           <int>
  ##> 1 SUNDANCE ENERGY     2
d0 %>%
	filter(name == "SUNDANCE ENERGY")
  ##>   ticker name  xsector xindustry mindustry date  usd   exch  SP500  gics
  ##>   <chr>  <chr>   <int>     <int>     <int> <chr> <chr> <chr> <int> <int>
  ##> 1 SDCJF  SUND…      12       136        42 01/3… AUS   OTC       0    10
  ##> 2 SNDE   SUND…      12       136        42 01/3… USA   NSDQ      0    10
``` 

Verify that the foreign keys are valid:

		`all_data$mindustry` to `industrie$M_Industry` 
		`all_data$xsector` to `sectors$xsector`

``` r
setdiff(d0$mindustry, i0$M_Industry)
  ##> empty
setdiff(d0$xsector, i0$xsector)
  ##> empty
``` 

### descriptive stats  id=g_10798


for categorical vars:

``` r
desc_cat(d0, c('SP500'))
  ##>   var     `0`   `1`    avg    sd missing     n
  ##> 1 SP500  92.2  7.78 0.0778 0.268       0  6492
``` 

for numeric vars:

``` r
desc_num(d0, c('marketValue'))
  ##>   var           avg      sd missing     n   min   med      max
  ##> 1 marketValue 7412. 178617.       0  6492     0  254. 14186708
``` 

Check values of other categorical fields:

``` R
unique(d0$xindustry) %>% sort
  ##>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  19
``` 

### Environment variable:

``` r
dbhost = Sys.getenv("DB_HOST")
``` 

### Read text into dataframe

https://stackoverflow.com/questions/3936285/is-there-a-way-to-use-read-csv-to-read-from-a-string-value-rather-than-a-file-in


``` r
lines <- "
+ flim,flam
+ 1.2,2.2
+ 77.1,3.14
+ "
``` 

opt01: `read.csv`

``` r
data <- read.csv(text=lines)
data
  ##>   flim flam
  ##> 1  1.2 2.20
  ##> 2 77.1 3.14
``` 

opt02: `textConnection`

``` r
con <- textConnection(lines)
data <- read.csv(con)
close(con)
data
  ##>   flim flam
  ##> 1  1.2 2.20
  ##> 2 77.1 3.14
``` 

## Use seq_along and seq_len in for loops

Use `seq_along` with list/vector objects instead of `1:length(vec)`. This won't run the loop when object is empty.

``` r
	for (gr in seq_along(route_groups)) {
	  print(gr)
	}
	##> no looping
``` 

Use `seq_len` for dataframes. This won't run the loop when there is no rows:

``` r
	for (sqn in seq_len(nrow(df))) {
		print(sqn)
	}
``` 

### replace_na

``` r
twc3 = twc %>%
	left_join(c0, by = "customer_id") %>%
	dplyr::mutate_at("customer_name", tidyr::replace_na, "Depo")
``` 

### mkdir mv ls directory commands

``` r
     file.create(..., showWarnings = TRUE)
     file.exists(...)
     file.remove(...)
     file.rename(from, to)
     file.append(file1, file2)
     file.copy(from, to, overwrite = recursive, recursive = FALSE,
               copy.mode = TRUE, copy.date = FALSE)
     file.symlink(from, to)
     file.link(from, to)
``` 

mkdir dir.create

``` r
dir.create(path = ... ) # mkdir
dir.create(path = ..., recursive = T) # mkdir -p
``` 

ls list.files

``` r
list.files(path = glue::glue("{PEYMAN_PROJECT_DIR}/pvrp_data/out"), include.dirs = T, pattern = "^report_\\d+.*")
``` 

mv move move_files rename files and directories

``` bash
file.rename(
	glue::glue("{pvrpr::PEYMAN_PROJECT_DIR}/pvrp/out")
  , glue::glue("{pvrpr::PEYMAN_PROJECT_DIR}/pvrp/out_{format(Sys.time(), '%Y%m%d_%H%M%S')}")
)
``` 

``` r
install.packages("filesstrings")
install.packages("processx")
filesstrings::move_files(
	glue::glue("{pvrpr::PEYMAN_PROJECT_DIR}/pvrp_data/out")
  , glue::glue("{pvrpr::PEYMAN_PROJECT_DIR}/pvrp_data/out_{format(Sys.time(), '%Y%m%d_%H%M%S')}")
)
``` 

### excel read/write

``` r
d0 = dplyr::tibble(a = 1:3, b = 10:12)
WriteXLS::WriteXLS(d0, "file01.xlsx", SheetNames = "d0")
rio::export(d0, "file.xlsx")

d1 = readxl::read_excel("file01.xlsx", sheet = "d0")
``` 

### run shinyApp

opt01: runApp from directory

``` r
shiny::runApp(".")
``` 

opt02: runApp from functions

``` r
runApp(shinyApp(ui, server), host="0.0.0.0",port=5050)
``` 

or

``` r
app = shiny::shinyApp(ui, server)
runApp(app)
``` 

### Convert sf to dataframe

https://gis.stackexchange.com/questions/224915/extracting-data-frame-from-simple-features-object-in-r

``` r
r0 = dplyr::select(state$routeSS, salesman_id, week_day) 
sf::st_geometry(r0) <- NULL
r0
``` 

### convert secs to hours

https://stackoverflow.com/questions/27312292/convert-seconds-to-days-hoursminutesseconds

``` r
library(lubridate)
seconds_to_period(86400)
  ##> #[1] "1d 0H 0M 0S"

seconds_to_period(48000)
  ##> #[1] "13H 20M 0S"
  ##> If you need to format

td <- seconds_to_period(86400)
sprintf('%02d %02d:%02d:%02d', day(td), td@hour, minute(td), second(td))
  ##> #[1] "01 00:00:00"
  ##> If it spans for >99 days,

td <- seconds_to_period(1e7)
sprintf('%03d %02d:%02d:%02d', day(td), td@hour, minute(td), second(td))
  ##> #[1] "115 17:46:40"


``` 

Print as hours

``` r
lubridate::seconds_to_period(sum(d2$sure, na.rm=T)) %>%
	lubridate::as.duration() %>%
	as.numeric("hours")
``` 

### create a package and use a library inside

``` r
library(usethis)
usethis::create_package("ex01")
usethis::use_package("MASS", "Suggests")
usethis::use_package("dplyr")
``` 

### package: export functions

``` r
#' @export
foo <- function(..)
``` 

Add symbols:

``` r
#' @importFrom magrittr "%>%"
NULL
``` 

## package: build and install

`Makefile`

``` txt
build:
	R -e 'devtools::document(); devtools::build_vignettes(); devtools::build(); devtools::install()'
``` 

``` bash
make build
``` 

### json jsonr examples

``` r
pj0 = rjson::fromJSON(file="logs/plan_jobs.json")
  ##>  $ :List of 11
  ##>   ..$ _id              :List of 1
  ##>   .. ..$ $oid: chr "591e95424eb11b0a51ac8712"
  ##>   ..$ depot            : chr "CORLU"
  ##>   ..$ status           : chr "TAMAMLANDI"
  ##>   ..$ submitDate       : chr "2017-05-19 09:48:33"
  ##>   ..$ user             : chr "plan_corlu"
  ##>  $ :List of 11
  ##>   ..$ _id              :List of 1
  ##>   .. ..$ $oid: chr "591ea8ba4eb11b0a51ac87ef"
  ##>   ..$ depot            : chr "ADANA"
length(pj0)
  ##> [1] 2432
``` 

Read as dataframe

``` r
pl0 = jsonlite::fromJSON("logs/plan_jobs.json", simplifyDataFrame = T) %>%
	as_tibble()
str(pl0)
  ##> 'data.frame':   2432 obs. of  12 variables:
pl0
  ##> # A tibble: 2,432 x 12
  ##>    `_id`$`$oid`   depot  shortStatus status  submitDate 
  ##>    <chr>          <chr>  <chr>       <chr>   <chr>      
  ##>  1 59259c9b4eb11… ADANA  G           TAMAML… 2017-05-2… 
  ##>  2 5926703e4eb11… CORLU  G           TAMAML… 2017-05-2… 
``` 

### group_by then count tally 

``` r
pl1 %>%
	count(depot)
  ##>   depot         n
  ##>   <chr>     <int>
  ##> 1 ADANA         7
  ##> 2 CORLU       136
  ##> 3 DENIZLI      65
  ##> 4 ESKISEHIR   137
``` 

Equivalent to:

``` r
pl1 %>%
	group_by(depot) %>%
	tally()
``` 

### grepl str_detect

``` r
"(ali)" %>% stringr::str_detect("^\\(.*\\)$")
  ##> [1] TRUE
``` 

### run r script from terminal

``` bash
Rscript --verbose --vanilla backtesting/backtesting.R
R CMD ... # deprecated
R --vanilla -e "source('~/projects/itr/vrp_doc/study/ex/peyman_data_relations_20190522/prepare_od_table.R'); test_origin_destination_json()"
R --no-save --no-restore-data
R --no-save --no-restore-data -e "source('~/projects/itr/vrp_doc/study/ex/peyman_data_relations_20190522/prepare_od_table.R'); test_origin_destination_json()"
``` 

### get version of a package

``` r
packageVersion("stringr")
``` 

### print/show all rows of a df

``` r
state$routes %>% print(n = 20)
``` 

### select a default CRAN mirror

https://stackoverflow.com/questions/11488174/how-to-select-a-cran-mirror-in-r

