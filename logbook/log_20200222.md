
# 20200224 

``` bash
rename 's/ \(z-lib.org\)//' *.*
rename 's/-+\d+-\(z-lib.org\)//' *.*
(by-Tyler-Cowen)-The-Age-of-the-Infovore-Succeedi-2729384-(z-lib.org).epub
``` 

## Tool: R deSolve ODE numerical simulator

Ref: `/Users/mertnuhoglu/Downloads/kitap/System Dynamics Modeling with R by Jim Duggan.pdf`

https://github.com/JimDuggan/SDMR

Check `<url:///~/codes/rr/SDMR/workshops/02 WPI 2017/models/01 SingleStock.R>`

# web 20200222 

## Tool: mdanki: convert markdown to anki cards

https://github.com/ashlinchak/mdanki

``` bash
mdanki ~/projects/study/logbook/anki_log_20200222.md ~/projects/study/logbook/anki_log_20200222.apkg
``` 

## Article: 40 powerful concepts

https://threadreaderapp.com/thread/1225561131122597896.html

Ref: `~/projects/study/logbook/anki_log_20200222.md`

Causal Reductionism: Things rarely happen for just 1 reason. Usually, outcomes result from many causes conspiring together. But our minds cannot process such a complex arrangement, so we tend to ascribe outcomes to single causes, reducing the web of causality to a mere thread. 

Ergodicity: A die rolled 100 times has equal probabilities to 100 dice rolled once; rolling a die is “ergodic”. But if the die gets chipped after 10 throws so it’s likelier to roll 4, then 1 die 100 times =/= 100 dice once (non-ergodic). Many treat non-ergodic systems as ergodic. 

Simpson’s Paradox: A trend can appear in groups of data but disappear when these groups are combined. This effect can easily be exploited by limiting a dataset so that it shows exactly what one wants it to show. Thus: beware of even the strongest correlations.

Focusing Illusion: Nothing is ever as important as what you’re thinking about while you’re thinking about it. E.g. worrying about a thing makes the thing being worried about seem worse than it is. As Marcus Aurelius observed, “We suffer more often in imagination that in reality.”

Concept Creep: As a social issue such as racism or sexual harassment becomes rarer, people react by expanding their definition of it, creating the illusion that the issue is actually getting worse.

Streetlight Effect: People tend to get their information from where it’s easiest to look.

The Petrie Multiplier: In fields in which men outnumber women, such as in STEM, women receive an underestimated amount of harassment due to the fact that there are more potential givers than receivers of harassment. (See also: Lotka–Volterra equations)

Tocqueville Paradox: As the living standards in a society rise, the people’s expectations of the society rise with it. The rise in expectations eventually surpasses the rise in living standards, inevitably resulting in disaffection

Ultimate Attribution Error: We tend to attribute good acts by allies to their character, and bad acts by allies to situational factors. For opponents, it’s reversed: good acts are attributed to situational factors, and bad acts to character

Golden Hammer: When someone, usually an intellectual who has gained a cultish following for popularizing a concept, becomes so drunk with power he thinks he can apply that concept to everything

Nirvana Fallacy: When people reject a thing because it compares unfavorably to an ideal that in reality is unattainable. E.g. condemning capitalism due to the superiority of imagined socialism, condemning ruthlessness in war due to imagining humane (but unrealistic) ways to win.

Anentiodromia: An excess of something can give rise to its opposite. E.g. A society that is too liberal will be tolerant of tyrants, who will eventually make it illiberal.

Halo Effect: When a person sees an agreeable characteristic in something or someone, they assume other agreeable characteristics. Example: if a Trump supporter sees someone wearing a MAGA cap, he’s likely to think that person is also decent, honest, hard-working, etc

Loki’s Wager: Fallacy where someone tries to defend a concept from criticism, or dismiss it as a myth, by unduly claiming it cannot be defined. E.g. “God works in mysterious ways” (god of the gaps), “race is biologically meaningless” (Lewontin’s fallacy).

Subselves: We use different mental processes in different situations, so each of us is not a single character but a collection of different characters, who take turns to commandeer the body depending on the situation. There is an office “you”, a lover “you”, an online “you”, etc

Goodhart’s Law: When a measure becomes a goal, it ceases to become a measure. E.g. British colonialists tried to control snakes in India. They measured progress by number of snakes killed, offering money for snake corpses. People responded by breeding snakes & killing them.

Radical Phase Transition (my term): Extremist movements can behave like solids (tyrannies), liquids (insurgencies), and gases (conspiracy theories). Pressuring them causes them to go from solid => liquid => gas. Leaving them alone causes them to go from gas => liquid => solid.

Legibility: We see a complex natural system, assume that because it *looks* messy that it must be disordered, then impose our own order on it to make it “legible”. But in removing the messiness we remove essential components of the system that we couldn’t grasp, and it fails.

Availability Cascade: When a new concept enters the arena of ideas, people react to it, thereby amplifying it. The idea thus becomes more popular, causing even more people to amplify it by reacting to it, until everyone feels the need to talk about it. 

Gurwinder Principle: It is often necessary to eat chocolate cake. 

Reactance Theory: When someone is restricted from expressing a POV, or pressured to adopt a different POV, they usually react by believing their original POV even more

Pareidolia: For aeons predators stalked us in undergrowth & shadow. In such times survival favored the paranoid—those who could discern a wolf from the vaguest of outlines. This paranoia preserved our species, but cursed us with pareidolia, so we now see wolves even in the skies.


## Article: 100 Little Ideas · Collaborative Fund

https://www.collaborativefund.com/blog/100-little-ideas/

Boomerang Effect: Trying to persuade someone to do one thing can make them more likely to do the opposite, because the act of persuasion can feel like someone stealing your freedom and doing the opposite makes you feel like you’re taking your freedom back.

Chronological Snobbery: “The assumption that whatever has gone out of date is on that account discredited. You must find why it went out of date. Was it ever refuted (and if so by whom, where, and how conclusively) or did it merely die away as fashions do?

Planck’s Principle: “A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die and a new generation grows up that is familiar with it.”

McNamara Fallacy: A belief that rational decisions can be made with quantitative measures alone, when in fact the things you can’t measure are often the most consequential. Named after Defense Secretary McNamara, who tried to quantify every aspect of the Vietnam War.

Courtesy Bias: Giving opinions that are likely to offend people the least, rather than what you actually believe.

Berkson’s Paradox: Strong correlations can fall apart when combined with a larger population. Among hospital patients, motorcycle crash victims wearing helmets are more likely to be seriously injured than those not wearing helmets. But that’s because most crash victims saved by helmets did not need to become hospital patients, and those without helmets are more likely to die before becoming a hospital patient.

Baader-Meinhof Phenomenon (Frequency Illusion): Noticing an idea everywhere you look as soon as it’s brought to your attention in a way that makes you overestimate its prevalence.


Ludic Fallacy: Falsely associated simulations with real life. Nassim Taleb: “Organized competitive fighting trains the athlete to focus on the game and, in order not to dissipate his concentration, to ignore the possibility of what is not specifically allowed by the rules, such as kicks to the groin, a surprise knife, et cetera. So those who win the gold medal might be precisely those who will be most vulnerable in real life.”

Normalcy Bias: Underestimating the odds of disaster because it’s comforting to assume things will keep functioning the way they’ve always functioned.

Actor-Observer Asymmetry: We judge others based solely on their actions, but when judging ourselves we have an internal dialogue that justifies our mistakes and bad decisions.

The 90-9-1 Rule: In social media networks, 90% of users just read content, 9% of users contribute a little content, and 1% of users contribute almost all the content. Gives a false impression of what ideas are popular or “average.”

Texas Sharpshooter Fallacy: Goals set retroactively after an activity, like shooting a blank wall and then drawing a bullseye around the holes you left, or picking a benchmark after you’ve invested.

Fredkin’s Paradox: Confronted with two equally good options, you struggle to decide, even though your decision doesn’t matter because both options are equally good. The more equal the options, the harder the decision.

Poisoning the Well: Presenting irrelevant adverse information about someone in a way that makes everything else that person says seem untrustworthy. “Before you hear my opponent’s healthcare plan, let me remind you that he got a DUI in college.”

Golem Effect: Performance declines when supervisors/teachers have low expectations of your abilities.

Appeal to Consequences: Arguing that a hypothesis must be true (or false) because the outcome is something you like (or dislike). The classic example is arguing that climate change isn’t real because combating climate change will hurt the economy.

Plain Folks Fallacy: People of authority acquiring trust by presenting themselves as Average Joe’s, when in fact their authority proves they are different from everyone else.

Behavioral Inevitability: “History never repeats itself; man always does.” – Voltaire

Apophenia: A tendency to perceive correlations between unrelated things, because your mind can only deal with tiny sample sizes and assuming things are correlated creates easy/comforting explanations of how the world works.

Self-Handicapping: Avoiding effort because you don’t want to deal with the emotional pain of that effort failing.

Hanlon’s Razor: “Never attribute to malice that which can be adequately explained by stupidity.”

False Uniqueness Effect: Assuming your skills are unique when they’re not. Comes from conflating “I’m good at this” with “Others are bad at this.”

Hard-Easy Effect: Hard tasks promote overconfidence because the rewards are high and fun to dream about; easy tasks promote underconfidence because they’re boring and easy to put off.

Neglect of Probability: Arguing that Nate Silver was wrong when he said Hillary Clinton has a 70% chance of winning, and using Donald Trump’s victory as your proof. Good predictions are based on probabilities, but the assessment of predictions are always binary, right or wrong.

Cobra Effect: Attempting to solve a problem makes that problem worse. Comes from an Indian story about a city infested with snakes offering a bounty for every dead cobra, which caused entrepreneurs to start breeding cobras for slaughter.

Braess’s Paradox: Adding more roads can make traffic worse because new shortcuts become popular and overcrowded.

Non-Ergodic: When group probabilities don’t apply to singular events. If 100 people play Russian Roulette once, the odds of dying might be, say, 10%. But if one person plays Russian Roulette 100 times, the odds are dying are practically 100%.

Pollyanna Principle: It’s easier to remember happy memories than bad ones.

Declinism: Perpetually viewing society as in decline, because you’re afflicted by the Pollyanna Principle and you forget how much things sucked in the past.

Empathy Gap: Underestimating how you’ll behave when you’re “hot” (angry/aroused/rushed), caused by the inability to accurately foresee how your body’s physical response to situations (dopamine, adrenaline, etc.) will influence decision-making.

Abilene Paradox: A group decides to do something that no one in the group wants to do because everyone mistakenly assumes they’re the only ones who object to the idea and they don’t want to rock the boat by speaking up.

Collective Narcissism: Exaggerating the importance and influence of your social group (country, industry, company, department, etc.).

Moral Luck: Praising someone for a good deed they didn’t have full control over. “Avoid calling heroes those who had no other choice.” – Taleb.

Feedback Loops: Falling stock prices scare people, which cause them to sell, which makes prices fall, which scares more people, which causes more people to sell, and so on. Works both ways.

Hawthorne Effect: Being watched/studied changes how people behave, making it difficult to conduct social studies that accurately reflect the real world.

Perfect Solution Fallacy: Comparing reality with an idealized alternative. Prevalent in any field governed by uncertainty.

Weasel Words: Phrases that appear to have meaning but convey nothing tangible. “Growth was solid last quarter,” or “Many people believe.”

Hormesis: Something that hurts you in a high dose can be good for you in small doses. (Weight on your bones, drinking red wine, etc.)

Backfiring Effect: A supercharged version of confirmation bias where being presented with evidence that goes against your beliefs makes you double down on your initial beliefs because you feel you’re being attacked.

Reflexivity: When cause and effect are the same. People think Tesla will sell a lot of cars, so Tesla stock goes up, which lets Tesla raise a bunch of new capital, which helps Tesla sell a lot of cars.

Second Half of the Chessboard: Put one grain of rice on the first chessboard square, two on the next, four on the next, then eight, then sixteen, etc, doubling the amount of rice on each square. When you’ve covered half the chessboard’s squares you’re dealing with an amount of rice that can fit in your lap; in the second half you quickly get to a pile that will consume an entire city. That’s how compounding works: slowly, then ferociously.

Peter Principle: Good workers will continue to be promoted until they end up in a role they’re bad at.

Friendship Paradox: On average, people have fewer friends than their friends have. Occurs because people with an abnormally high number of friends are more likely to be one of your friends. It’s a fundamental part of social network dynamics and makes most people feel less popular than they are.

Hedonic Treadmill: Expectations rise with results, so nothing feels as good as you’d imagine for as long as you’d expect.

Positive Illusions: Excessively rosy views about the decisions you’ve made to maintain self-esteem in a world where everyone makes bad decisions all the time.

Ironic Process Theory: Going out of your way to suppress thoughts makes those thoughts more prominent in your mind.

Clustering Illusions: Falsely assuming that the inevitable bunching of random results in a large sample indicates a trend.

Foundational Species: A single thing that plays an outsized role in supporting an ecosystem, whose loss would pull down many others with it. In nature: kelp, algae, and coral. In business: The Federal Reserve and Amazon.

Bizarreness Effect: Crazy things are easier to remember than common things, providing a distorted sense of “normal.”

Nonlinearity: Outputs aren’t always proportional to inputs, so the world is a barrage of massive wins and horrible losses that surprise people.

Moderating Relationship: The correlation between two variables depends on a third, seemingly unrelated variable. The quality of a marriage may be dependent on a spouse’s work project that’s causing stress.

Denomination Effect: One hundred $1 bills feels like less money than one $100 bill. Also explains stock splits – buying 10 shares for $10 each feels cheaper than one share for $100.

Woozle Effect: “A reliable way to make people believe in falsehoods is frequent repetition, because familiarity is not easily distinguished from truth.” - Daniel Kahneman.

Google Scholar Effect: Scientific research depends on citing other research, and the research that gets cited the most is whatever shows up in the top results of Google Scholar searches, regardless of its contribution to the field.

Inversion: Avoiding problems can be more important than scoring wins.

Gambler’s Ruin: Has many meanings, the most important of which is that playing a negative-probability game persistently enough guarantees going broke.

Principle of Least Effort: When seeking information, effort declines as soon as the minimum acceptable result is reached.

Dunning-Kruger Effect: Knowing the limits of your intelligence requires a certain level of intelligence, so some people are too stupid to know how stupid they are.

Knightian Uncertainty: Risk that can’t be measured; admitting that you don’t know what you don’t know.

Aumann’s Agreement Theorem: If you understand your opponent’s beliefs you cannot agree to disagree. If you agree to disagree it’s because one side doesn’t understand the other side’s view.

Focusing Effect: Overemphasizing factors that seem important but exist as part of a complex system. People from the Midwest assume Californians are happier because the weather is better, but they’re not because Californians also deal with traffic, bad bosses, unhappy marriages, etc, which more than offset the happiness boost from sunny skies.

The Middle Ground Fallacy: Falsely assuming that splitting the difference between two polar opposite views is a healthy compromise. If one person says vaccines cause autism and another person says they don’t, it’s not right to compromise and say vaccines sometimes cause autism.

Rebound Effect: New symptoms, or supercharged old symptoms, emerge when medicine or other protections are withdrawn.

Ostrich Effect: Avoiding negative information that might challenge views that you desperately want to be right.

Founder’s Syndrome: When a CEO is so emotionally invested in a company that they can’t effectively delegate decisions.

In-Group Favoritism: Giving preference to people from your social group regardless of their objective qualifications.

Bounded Rationality: People can’t be fully rational because your brain is a hormone machine, not an Excel spreadsheet.

Luxury Paradox: The more expensive something is the less likely you are to use it, so the relationship between price and utility is an inverted U. Ferraris sit in garages; Hondas get driven.

Meat Paradox: Dogs are family, pigs are food. Some animals classified as food are wrongly perceived to have lower intelligence than those classified as pets. An example of morality depending on utility.

Fluency Heuristic: Ideas that can be explained simply are more likely to be believed than those that are complex, even if the simple-sounding ideas are nonsense. It occurs because ideas that are easy to grasp are hard to distinguish from ideas you’re familiar with.

Historical Wisdom: “The dead outnumber the living 14 to 1, and we ignore the accumulated experience of such a huge majority of mankind at our peril.” – Niall Ferguson

Fact-Check Scarcity Principle: This article is called 100 Little Ideas but there are fewer than 100 ideas. Most of readers won’t notice because they’re not checking, and most of those who notice won’t say anything. Don’t believe everything you read.

Emotional Contagion: One person’s emotions trigger the same emotions in other people, because evolution has selected for empathizing with those in your social group whose actions you rely on.

Tribal Affiliation: Beliefs can be swayed by identity and a desire to fit in over rational analysis. There is little correlation between climate change denial and scientific literacy. But there is a strong correlation between climate change denial and political affiliation.

Emotional Competence: The ability to recognize others’ emotions and respond to them productively. Harder and rarer than it sounds.

## Other Concepts

https://en.wikiquote.org/wiki/Bounded_rationality

Bounded rationality is the idea that in decision-making, rationality of individuals is limited by the information they have, the cognitive limitations of their minds, and the finite amount of time they have to make a decision.

Bounded rationality means that people make quite reasonable decisions based on the information they have. But they don't have perfect information, especially about more distant parts of the system. [...] We don't even interpret perfectly the imperfect information that we do have, say behavioral scientists. [...] Which is to say, we don't even make decisions that optimize our own individual good, much less the good of the system as a whole.

