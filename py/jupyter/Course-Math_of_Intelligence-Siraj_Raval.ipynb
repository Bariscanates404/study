{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Video 01: Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "video: https://www.youtube.com/watch?v=xRJCOz3AfYY&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D\n",
    "\n",
    "source code: https://github.com/llSourcell/Intro_to_the_Math_of_intelligence/blob/master/demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "gradient descent for line fitting:\n",
    "\n",
    "![gradient descent for line fitting](https://raw.githubusercontent.com/mattnedrich/GradientDescentExample/master/gradient_descent_example.gif \"Visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sum of Squares Error Function\n",
    "\n",
    "$$sse(m,b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (m x_i + b))^2 $$\n",
    "\n",
    "We need to minimize $sse$ wrt $m$ and $b$.\n",
    "\n",
    "Thus we take derivative of $sse$ wrt $m$ and $b$ and use gradient descent to approach to zero slope.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial sse}{\\partial m} & = \\frac{2}{n} \\sum_i^n - x_i (y_i - (m x_i + b)) \\\\\n",
    "\\frac{\\partial sse}{\\partial b} & = \\frac{2}{n} \\sum_i^n - (y_i - (m x_i + b)) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that gradients depend on n points: $(x_i, y_i)$\n",
    "\n",
    "So, the code to calculate gradients will loop over all points:\n",
    "\n",
    "    def step_gradient(b_current, m_current, points, learningRate):\n",
    "        b_gradient = 0\n",
    "        m_gradient = 0\n",
    "        N = float(len(points))\n",
    "        for i in range(0, len(points)):\n",
    "            x = points[i, 0]\n",
    "            y = points[i, 1]\n",
    "            b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "            m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run(points):\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    return [b,m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.088936519937413458, 1.4777440851894448]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = genfromtxt(\"../../../study_data/math_of_intelligence/01_data.csv\", delimiter=\",\")\n",
    "run(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Video 02: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "use cases for svm:\n",
    "\n",
    "- classification, regression, outlier detection, clustering\n",
    "\n",
    "### Comparison to other methods\n",
    "\n",
    "svm good when:\n",
    "\n",
    "- small set of data (<1000 rows)\n",
    "\n",
    "other algorithms (random forest, dnn etc) \n",
    "\n",
    "- more data \n",
    "- always very robust\n",
    "\n",
    "Knuth: \"Premature optimization is the root of all evil in programming\"\n",
    "\n",
    "### What is svm?\n",
    "\n",
    "a discriminative classifier\n",
    "\n",
    "opposite to discriminative: generative. it generates new data.\n",
    "\n",
    "svm tries to maximise the gap between two classes of points\n",
    "\n",
    "these points: support vectors\n",
    "\n",
    "hyperplane: the line that separates the two classes of points\n",
    "\n",
    "hyperplane is (n-1) dimensional in $\\rm I\\!R^n$\n",
    "\n",
    "### Linear vs nonlinear classification\n",
    "\n",
    "to do non-linear classification: kernel trick\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loss function\n",
    "\n",
    "Minimize loss function to maximize the discrimination.\n",
    "\n",
    "Hinge loss function: for maximum margin classification\n",
    "\n",
    "$$c(x,y,f(x)) = (1 - y \\times f(x))_+$$\n",
    "\n",
    "c is the loss function. x the sample. y true label. f(x) predicted label.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "c(x,y,f(x)) = \n",
    "\\begin{cases}\n",
    "0,& \\text{if } y \\times f(x) \\geq 1 \\\\\n",
    "1 - y \\times f(x),& \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### Objective function\n",
    "\n",
    "$$min_w \\lambda \\| w \\|^2 + \\sum_{i=1}^n (1-y_i \\langle x_i, w \\rangle )_+$$\n",
    "\n",
    "note that: $w = f(x)$\n",
    "\n",
    "objective function = regularizer + hinge loss\n",
    "\n",
    "what does regularizer do?\n",
    "\n",
    "- too high => overfit\n",
    "- too low => underfit\n",
    "\n",
    "so, regularizer controls trade off between low training error and low testing error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
